{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import robotparser\n",
    "\n",
    "# 1.DEFINICIÓN\n",
    "# Definimos la web que vamos a analizar y la principal\n",
    "url = 'https://retina.elpais.com/tag/iniciativa_empresarial/a/'\n",
    "url_main = 'https://retina.elpais.com/'\n",
    "\n",
    "# Definimos una cabecera con el user agent que obtenemos al ejecutar manualmente. \n",
    "# Lo utilizaremos como parámetro al llamar a las url.\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\"}\n",
    "\n",
    "# 2.CHEQUEAMOS ROBOTS.TXT Y CAMBIAMOS USER AGENT\n",
    "def check_robots(url_main):\n",
    "    rp = robotparser.RobotFileParser()    \n",
    "    rp.set_url(url_main + \"robots.txt\")\n",
    "    req_read = rp.read()\n",
    "    check = rp.can_fetch(headers.values(), url_main)\n",
    "    return check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que contiene el código para hacer el web scraping de la url con el useragent definido en headers\n",
    "def retina_scrap(url, headers):\n",
    "    # Definimos tuplas para cada columna del csv que vamos a crear.\n",
    "    title = []\n",
    "    link = []\n",
    "    date = []\n",
    "    section = []\n",
    "    autor = []\n",
    "    # 3.PAGINADO\n",
    "    # Buscamos el botón \">\" para ver el número de páginas que tiene la sección.Está ubicado en:\n",
    "    # <li class=\"paginacion-siguiente activo\"><a href=\"XXXXXXX\">Siguiente ›</a></li\n",
    "    res_pag = requests.get(url, headers=headers)\n",
    "    soup_pag = BeautifulSoup(res_pag.text, 'lxml')\n",
    "    link_pag = soup_pag.find('li', {'class': 'paginacion-siguiente activo'}).a['href'] \n",
    "    \n",
    "    # Al final de la cadena del link está el número de páginas que tiene la sección\n",
    "    num_pag = int(link_pag.split('/')[-1])\n",
    "\n",
    "    # 4.BÚSQUEDA\n",
    "    # Recorremos cada una de las páginas \n",
    "    index = range(num_pag+1)\n",
    "    for number in index:\n",
    "        # Llamamos a cada uno de las páginas.\n",
    "        res = requests.get(url+str(number), headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "        \n",
    "        # Buscamos el título de cada artículo y el link\n",
    "        for i in soup.find_all('h2', {'class': 'articulo-titulo'}):\n",
    "            # Título\n",
    "            new_title = i.get_text()\n",
    "            title.append(new_title)\n",
    "            \n",
    "            # Link al artículo\n",
    "            article_link = i.a['href']\n",
    "            new_link = \"https://retina.elpais.com\"+article_link\n",
    "            link.append(new_link)\n",
    "            \n",
    "            # Dentro del link buscamos la fecha, el autor y la sección\n",
    "            res_link = requests.get(new_link, headers=headers)\n",
    "            soup_link = BeautifulSoup(res_link.text, 'lxml')\n",
    "        \n",
    "            # Fecha\n",
    "            new_date = soup_link.time['datetime']\n",
    "            date.append(new_date)\n",
    "            \n",
    "            # Autor (controlamos si no existe)\n",
    "            new_autor = soup_link.find('div', {'class':'firma'})\n",
    "            if not new_autor:\n",
    "                new_autor = 'No autor'\n",
    "            else:\n",
    "                new_autor = new_autor.a.get_text()\n",
    "            autor.append(new_autor.replace(\"\\n\", \"\"))\n",
    "        \n",
    "            # Sección (controlamos si no existe)\n",
    "            new_section = soup_link.find('div', {'class':'articulo-antetitulo'})\n",
    "            if not new_section:\n",
    "                new_section = 'no section'\n",
    "            else:\n",
    "                new_section = new_section.get_text()\n",
    "            section.append(new_section.replace(\"\\n\", \"\"))\n",
    "    \n",
    "    # 5.GENERAR CSV\n",
    "    # Definimos un dataframe con la información extraida de la web\n",
    "    data = {'Title':title, \n",
    "        'PageLink':link,\n",
    "        'Date':date,\n",
    "        'Section':section,\n",
    "        'Author':autor\n",
    "        }\n",
    "    news = pd.DataFrame(data=data)\n",
    "    cols = ['Title', 'PageLink', 'Date', 'Section', 'Author']\n",
    "    news = news[cols]\n",
    "\n",
    "    return news\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifica que el useragent esté permitido en el robots.txt \n",
    "ua_check = check_robots(url)\n",
    "if ua_check == True: \n",
    "    #ejecuta la función para obtener el dataset\n",
    "    retina_scrap(url, headers)\n",
    "    # Generamos el csv retina_news.csv\n",
    "    filename = 'scraping_news.csv'\n",
    "    news.to_csv(filename, index=False, encoding='utf-8-sig', header=True, sep='|')\n",
    "\n",
    "else:\n",
    "    \"There is a problem for scraping with current useragent\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
