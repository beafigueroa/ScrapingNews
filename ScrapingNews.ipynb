{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Agent: *\r\n",
      "Disallow: /buscador/\r\n",
      "Disallow: /pruebas/\r\n",
      "Disallow: /publicidad/\r\n",
      "Disallow: /notificarelacionadas\r\n",
      "Disallow: /*.swf$\r\n",
      "Disallow: /eskupTSUpdate\r\n",
      "Disallow: /.well-known/amphtml/\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import urllib \n",
    "import io\n",
    "\n",
    "# 1.DEFINICIÓN\n",
    "# Definimos la web que vamos a analizar y la principal\n",
    "url = 'https://retina.elpais.com/tag/iniciativa_empresarial/a/'\n",
    "url_main = 'https://retina.elpais.com/'\n",
    "\n",
    "# Definimos tuplas para cada columna del csv que vamos a crear.\n",
    "title = []\n",
    "link = []\n",
    "date = []\n",
    "section = []\n",
    "autor = []\n",
    "\n",
    "# 2.CHEQUEAMOS ROBOTS.TXT Y CAMBIAMOS USER AGENT\n",
    "def check_robots(url_main):\n",
    "    req = urllib.urlopen(url_main + \"robots.txt\", data = None)\n",
    "    print req.read()\n",
    "    return req.read()\n",
    "\n",
    "check_robots(url_main)\n",
    "\n",
    "# Definimos una cabecera con el user agent que obtenemos al ejecutar manualmente. \n",
    "# Lo utilizaremos como parámetro al llamar a las url.\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\"}\n",
    "\n",
    "# 3.PAGINADO\n",
    "# Buscamos el botón \">\" para ver el número de páginas que tiene la sección.Está ubicado en:\n",
    "# <li class=\"paginacion-siguiente activo\"><a href=\"XXXXXXX\">Siguiente ›</a></li\n",
    "res_pag = requests.get(url, headers=headers)\n",
    "soup_pag = BeautifulSoup(res_pag.text, 'lxml')\n",
    "link_pag = soup_pag.find('li', {'class': 'paginacion-siguiente activo'}).a['href'] \n",
    "\n",
    "# Al final de la cadena del link está el número de páginas que tiene la sección\n",
    "num_pag = int(link_pag.split('/')[-1])\n",
    "\n",
    "# 4.BÚSQUEDA\n",
    "# Recorremos cada una de las páginas \n",
    "index = range(num_pag+1)\n",
    "for number in index:\n",
    "    \n",
    "    # Llamamos a cada uno de las páginas.\n",
    "    res = requests.get(url+str(number), headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "  \n",
    "    # Buscamos el título de cada artículo y el link\n",
    "    for i in soup.find_all('h2', {'class': 'articulo-titulo'}):\n",
    "        \n",
    "        # Título\n",
    "        new_title = i.get_text()\n",
    "        title.append(new_title)\n",
    "        \n",
    "        # Link al artículo\n",
    "        article_link = i.a['href']\n",
    "        new_link = \"https://retina.elpais.com\"+article_link\n",
    "        link.append(new_link)\n",
    "        \n",
    "        # Dentro del link buscamos la fecha, el autor y la sección\n",
    "        res_link = requests.get(new_link, headers=headers)\n",
    "        soup_link = BeautifulSoup(res_link.text, 'lxml')\n",
    "        \n",
    "        # Fecha\n",
    "        new_date = soup_link.time['datetime']\n",
    "        date.append(new_date)\n",
    "        \n",
    "        # Autor (controlamos si no existe)\n",
    "        new_autor = soup_link.find('div', {'class':'firma'})\n",
    "        if not new_autor:\n",
    "            new_autor = 'No autor'\n",
    "        else:\n",
    "            new_autor = new_autor.a.get_text()\n",
    "        autor.append(new_autor.replace(\"\\n\", \"\"))\n",
    "        \n",
    "        # Sección (controlamos si no existe)\n",
    "        new_section = soup_link.find('div', {'class':'articulo-antetitulo'})\n",
    "        if not new_section:\n",
    "            new_section = 'no section'\n",
    "        else:\n",
    "            new_section = new_section.get_text()\n",
    "        section.append(new_section.replace(\"\\n\", \"\"))\n",
    "\n",
    "# 5.GENERAR CSV\n",
    "# Definimos un dataframe con la información extraida de la web\n",
    "data = {'Title':title, \n",
    "        'PageLink':link,\n",
    "        'Date':date,\n",
    "        'Section':section,\n",
    "        'Author':autor\n",
    "        }\n",
    "news = pd.DataFrame(data=data)\n",
    "cols = ['Title', 'PageLink', 'Date', 'Section', 'Author']\n",
    "news = news[cols]\n",
    "\n",
    "# Generamos el csv retina_news.csv\n",
    "filename = 'scraping_news.csv'\n",
    "news.to_csv(filename, index=False, encoding='utf-8-sig', header=True, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
